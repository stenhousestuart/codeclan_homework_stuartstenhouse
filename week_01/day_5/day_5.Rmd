---
title: "Week 1 / Day 5 Homework"
output: html_notebook
---

# Library Setup 
```{r}
library(tidyverse)
library(janitor)
```

# Investigate Data

## Read in Data
```{r}
books <- read_csv("data/books.csv")
```

## Explore Data
```{r}
# View Number of Observations and Variables
dim(books)
# View Variable Names
names(books)
# View Variable Types
glimpse(books)
# View Data Visually
view(books)
# View Small Sample of Observations 
head(books)
tail(books)
```

# Clean Data

## Standardise Variable Names

```{r}
# Standardise Variable names.

books %>% 
  clean_names() %>% 
  names()

```

```{r}
# Manually rename variable names where appropriate.

books %>% 
  rename("row_id" = "rowid") %>% 
  names()

```
## Tidy Observation Data

### Publication Date

When initially trying to complete the next step of formatting the `publication_date` and extracting a year an error was returned that two values had failed to parse. I identified these by filtering the newly created `year` value for NAs and identified the two books with the error. In both cases the original `publication_date` looked to be invalid - as there as only 30 days in both November and June. Therefor, in this case, I recoded these dates to the 30th of the month and year specified instead of the 31st.

```{r}
# Recode 2x invalid dates.

books_corrected_dates <- books %>%
  mutate(publication_date = recode(publication_date, "11/31/2000" = "11/30/2000", "6/31/1982" = "6/30/1982"))

```

```{r}
# Use lubridate package to format `publicaton_date` and create a `year` variable.

books_date_cleaned <- books_corrected_dates %>% 
  mutate(publication_date = mdy(publication_date),
         publication_year = year(publication_date))

```

To check that this had been succesfful, I filtered the `publication_year` column for NAs and 0 were returned.

```{r}

books_date_cleaned %>% 
  filter(is.na(publication_year))

```

## Missing Values

### Check for NAs

```{r}
books_date_cleaned %>% 
  summarise(across(.fns = ~sum(is.na(.x))))
```

No NAs were identified, so I then checked for non-standard NAs - by checking for `0` in `dbl` variables and `NA` and `na` strings in `chr` variables.

### Check for Non-Standard NAs

```{r}
books_date_cleaned %>% 
  summarise(title_na = sum(title == "na"),
            title_NA = sum(title == "NA"),
            authors_na = sum(title == "na"),
            authors_NA = sum(title == "NA"),
            avg_rating_0 = sum(average_rating == 0),
            pages_0 = sum(num_pages == 0),
            ratings_count_0 = sum( ratings_count == 0),
            text_reviews_count_0 = sum(text_reviews_count == 0),
            publication_date_0 = sum(publication_date == 0),
            publication_year = sum(publication_year == 0),
            publisher_na = sum(title == "na"),
            publisher_NA = sum(title == "NA")
            )
```

#### Publication Date

1 observation returned a `0` for the `publication_date`. I checked this below but was unable to see why this was returned. As the `year` variable appeared to have populated correctly and it was this variable that I intend to use in analysis, I decided no further action was needed.

```{r}
books_date_cleaned %>% 
  select(title, publication_date, publication_year) %>% 
  filter(publication_date == 0)
```

Below I have detailed my approach to the observations that returned non-standard missing values.

#### Ratings
25 books have an `average_rating` of 0; however, 80 books have a `ratings_count` of 0. This means that some books have a `average_rating` value despite having `ratings_count`.

Approach: Where `average_rating` = 0, filter out observation.
Approach: Where `ratings_count` = 0, filter out observation.

I chose this approach rather than investigaing these further in order to utilise as wid a range of week 1 skills as possible, knowing that I intended to impute `num_pages` data.

```{r}
# Filter out observations with 0 values in `average_rating` and `ratings_count` variables.

books_ratings_cleaned <- books_date_cleaned %>% 
  filter(average_rating != 0,
         ratings_count != 0)

```

```{r}
# Check that filtering has worked.

books_ratings_cleaned %>% 
  summarise(average_rating_0 = sum(average_rating == 0),
            average_rating_na = sum(is.na(average_rating)),
            ratings_count_0 = sum(ratings_count == 0),
            ratings_count_na = sum(is.na(ratings_count))
            )
```

#### Pages
76 books have 0 `num_pages` which is not possible.

Approach: Where `num_pages` = 0, impute median `num_pages` from across all observations.

Other approaches could include imputing alternative data eg. mean.

```{r}
# Code for Imputing mean(num_pages) to replace observations with 0 pages.

books_pages_cleaned <- books_ratings_cleaned %>% 
  mutate(num_pages = recode(num_pages, "0" = median(num_pages)))

```

```{r}
# Check that no pages have a 0 value any more.

books_pages_cleaned %>% 
  summarise(num_pages_0 = sum(num_pages == 0))

```

## Consolidate Cleaned Data

To consolidate the cleaned data I assigned this to a single object that would be used moving forward.

```{r}

books_clean <- books_pages_cleaned

```

# Question 1: What are the top 5 "hits" and "flops" by authors, based on the difference in a books rating and the authors average rating.

Thoughts from Findings Below: Mary B. Collins appears to have had varied ratings, with both the biggest "hit" which was `1.93` higher than her average and the biggest "flop" which was `-1.93` less than her average.

```{r}
books_with_rating_differences <- books_clean %>% 
  group_by(authors) %>% 
# Create a new variable containing an authors average rating.
  mutate(authors_average_rating = mean(average_rating), 
         .after = average_rating) %>%
# Create a new variable containing the difference between a books rating and the authors  authors average rating.
  mutate(rating_difference = average_rating - authors_average_rating, 
         .after = authors_average_rating) %>%
# Ungroup the data to enable analysis of individual observations. 
  ungroup()

books_with_rating_differences %>%
  select(title, authors, average_rating, 
         authors_average_rating, rating_difference) %>% 
  slice_max(rating_difference, n = 5)

books_with_rating_differences %>%
  select(title, authors, average_rating, 
         authors_average_rating, rating_difference) %>% 
  slice_min(rating_difference, n = 5)
```

# Question 2: Who are the most popular authors of all time based an a mean of their avg. rating across all of their books?

Thoughts from Findings Below: Unfortunately as the authors are stored in a way that they are grouped in with other others, it is difficult to draw too many conclusions from this. I'm looking forward to learning more about cleaning and manipulating strings to support analysis in similar cases. For now, however; it has highlighted some previously un-identified issues with the authors data (eg. authors with the values as "Anonymous", various formatting of J.K. Rowling). These would need to be considered and cleaned for a more insightful analysis of the data.

```{r}
books_clean %>%
  group_by(authors) %>% 
  summarise(authors_average_rating = mean(average_rating)) %>% 
  select(authors, authors_average_rating) %>% 
  arrange(desc(authors_average_rating))
```

# Question 3: Which 10 years have the highest mean avg. rating?

Thoughts from Findings Below: Only 2 years since the year 2000 feature in the top 10. This raises some interesting questons as to why this might be?

a) Are critics becoming more strict in their ratings?
b) Is it easier to publish a book now (eg. through self-publishing, technology), meaning more books are being published and perhaps without the same quality controls?
c) What books were released in 2013 and 2016 that contributed to these being the only 2 years since 2000 that featured in the top 10 based on the average of average ratings?

Points b and c will be followed up on in `Question 4`.

```{r}
books_clean %>% 
  group_by(publication_year) %>% 
  summarise(year_average_rating = mean(average_rating)) %>% 
  select(publication_year, year_average_rating) %>% 
  arrange(desc(year_average_rating))

```

# Question 4a: Is it easier to publish a book now (eg. through self-publishing, technology), meaning more books are being published and perhaps without the same quaility controls?

Thoughts from Findings Below: The top 4 years with the highest ratings all only include 1 book in the dataset - meaning that this is the cause of the high `year_average_rating` rather than the hypothesis in the question.

```{r}

books_clean %>% 
  group_by(publication_year) %>% 
  summarise(year_average_rating = mean(average_rating),
         year_num_books = n()) %>% 
  select(publication_year, year_average_rating, year_num_books) %>%
  arrange(desc(year_average_rating))
  
```


# Question 4b: Were there any particular books released in 2013 and 2016 that contributed to these being the only 2 years since 2000 that featured in the top 10 based on the average of average ratings?

Thoughts from Findings Below: There are no particular books in these years that look to have contributed to a high `year_average_rating` - instead various books look to have contributed. 

```{r}
books_2013_2016 <- books_clean %>% 
  filter(publication_year == 2013 | publication_year == 2016) %>%
  select(publication_year, title, average_rating) %>% 
  arrange(desc(publication_year), desc(average_rating))

books_2013_2016
  
```


# Question 5: What are the the top 5 languages books have been published in, excluding English (GB or US).

Thoughts from Findings Below: Spanish and French language books are the most common in the dataset, after English.

```{r}
# Using fliter() and grepl() filter the data leaving only non-English language books.

books_filtered <- books_clean %>% 
  filter(!grepl('en|eng-US', language_code))

books_filtered %>% 
  group_by(language_code) %>% 
  summarise(total_books_by_language = n()) %>%
  select(language_code,total_books_by_language) %>% 
  slice_max(total_books_by_language, n = 5)

```
