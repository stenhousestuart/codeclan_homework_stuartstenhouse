---
title: "Week 1 / Day 5 Homework"
output: html_notebook
---

# Library Setup 
```{r}
library(tidyverse)
library(janitor)
```

# Investigate Data

## Read in Data
```{r}
books <- read_csv("data/books.csv")
```

## Explore Data
```{r}
# View Number of Observations and Variables
dim(books)
# View Variable Names
names(books)
# View Variable Types
glimpse(books)
# View Data Visually
view(books)
# View Small Sample of Observations 
head(books)
tail(books)
```

# Clean Data

## Standardise Variable Names

```{r}
# Standardise Vvariable names.

books %>% 
  clean_names() %>% 
  names()

```

```{r}
# Manually rename variable names where appropriate.

books %>% 
  rename("row_id" = "rowid") %>% 
  names()

```
## Tidy Observation Data

### Publication Date

When initially trying to complete the next step of formatting the `publication_date` and extracting a year an error was returned that two values had failed to parse. I identified these by filtering the newly created `year` value for NAs and identified the two books with the error. In both cases the original `publication_dates` looked to be invalid - as there as only 30 days in both November and June. Therefor, in this case, I recoded these dates to the 30th of the month and year specified instead of the 31st.

```{r}
# Recode 2x invalid dates.

books_corrected_dates <- books %>%
  mutate(publication_date = recode(publication_date, "11/31/2000" = "11/30/2000", "6/31/1982" = "6/30/1982"))
```

```{r}
# Use lubridate package to format `publicaton_date` format + create new `year` variable with the year only.

books_date_cleaned <- books_corrected_dates %>% 
  mutate(publication_date = mdy(publication_date),
         publication_year = year(publication_date))
```

To check that this had been succesfful, I filtered the `year` column for NAs and 0 were returned.

```{r}
# Check NA variable for NAs.

books_date_cleaned %>% 
  filter(is.na(publication_year))
```

## Missing Values

### Check for NAs

```{r}
books_date_cleaned %>% 
  summarise(across(.fns = ~sum(is.na(.x))))
```

No NAs were identified, so I then checked for non-standard NAs - by checking for `0` in `dbl` variables and `NA` and `na` sring values in `chr` variables.

### Check for Non-Standard NAs

```{r}
books_date_cleaned %>% 
  summarise(title_na = sum(title == "na"),
            title_NA = sum(title == "NA"),
            authors_na = sum(title == "na"),
            authors_NA = sum(title == "NA"),
            avg_rating_0 = sum(average_rating == 0),
            pages_0 = sum(num_pages == 0),
            ratings_count_0 = sum( ratings_count == 0),
            text_reviews_count_0 = sum(text_reviews_count == 0),
            publication_date_na = sum(publication_date == 0),
            publisher_na = sum(title == "na"),
            publisher_NA = sum(title == "NA")
            )
```

#### Publication Date

1 observation returned a `0` for the `publication_date`. I checked this below but was unable to see why this was returned. As the `year` variable appeared to have populated correctly and it was this variable that I intend to use in analysis, I decided no further action was needed.

```{r}
books_date_cleaned %>% 
  select(title, publication_date, publication_year) %>% 
  filter(publication_date == 0)
```

Below I have detailed my approach to the observations that have returned non-standard missing values.

#### Ratings
25 books have an `average_rating` of 0; however, 80 books have a `ratings_count` of 0. This means that some books have a `average_rating` value despite having `ratings_count` = 0 indicating incorrect data.

Approach: Where `average_rating` data is being used and `average_rating` = 0, filter out observation.
Approach: Where `ratings_count` data is being used and `ratings_count` = 0, filter out observation.

Other approaches would be to drop these observations or investigate further; however, the above approach has been taken to taken to utilise skills learned during week 1.

```{r}
# Filter out observations with 0 values in `average_rating` and `ratings_count` variables.

books_ratings_cleaned <- books_date_cleaned %>% 
  filter(average_rating != 0,
         ratings_count != 0)

```

```{r}
# Check that filtering has worked.

books_ratings_cleaned %>% 
  summarise(average_rating_0 = sum(average_rating == 0),
            average_rating_na = sum(is.na(average_rating)),
            ratings_count_0 = sum(ratings_count == 0),
            ratings_count_na = sum(is.na(ratings_count))
            )
```

#### Pages
76 books have 0 `num_pages` which is not possible.

Approach: Where `num_pages` = 0, impute median `num_pages` from across all observations.

Other approaches could include imputing alternative data eg. mean.

```{r}
# Code for Imputing mean(num_pages) to replace observations with 0 pages.

books_pages_cleaned <- books_ratings_cleaned %>% 
  mutate(num_pages = recode(num_pages, "0" = median(num_pages)))

```

```{r}
# Check that no pages have a 0 value any more.

books_pages_cleaned %>% 
  summarise(num_pages_0 = sum(num_pages == 0))

```

## Consolidate Cleaned Data

To consolidate the cleaned data I assigned this to a single object that would be used moving forward.

```{r}
# Assign cleaned data to appropriately named object for further analysis.

books_clean <- books_pages_cleaned

```

# Question 1: What are the top 5 "hits" and "flops" by authors, based on the difference in a books rating and the authors average rating.

Thoughts from Findings Below: Mary B. Collins appears to have had varied ratings, with both the biggest hit which was `1.93` higher than her average and the biggest flop which was `-1.93` less than her average.

```{r}
books_with_rating_differences <- books_clean %>% 
  group_by(authors) %>% 
  mutate(authors_average_rating = mean(average_rating), 
         .after = average_rating) %>%
  mutate(rating_difference = average_rating - authors_average_rating, 
         .after = authors_average_rating) %>%
  ungroup()

books_with_rating_differences %>%
  select(title, authors, average_rating, 
         authors_average_rating, rating_difference) %>% 
  slice_max(rating_difference, n = 5)

books_with_rating_differences %>%
  select(title, authors, average_rating, 
         authors_average_rating, rating_difference) %>% 
  slice_min(rating_difference, n = 5)
```

# Question 2: Who are the most popular authors of all time based an a mean of their avg. rating across all of their books?

Thoughts from Findings Below: Unfortunately as the authors are stored in a way that they are grouped in with other others, it is difficult to draw too many conclusions from this. It has however highlighted some additional issues with the authors data, showing "Anonymous" and "Cook's Illustrated Magazine" as author values. Therefore, these values and any other invalid author values would need to be considered for a more throrough analysis.

```{r}
books_clean %>%
  group_by(authors) %>% 
  summarise(authors_average_rating = mean(average_rating)) %>% 
  select(authors, authors_average_rating) %>% 
  arrange(desc(authors_average_rating))
```

# Question 3: Which 10 years have the highest mean avg. rating?

Thoughts from Findings Below: Since the year 2000, only 2 years feature in the top 10! Are books getting worse or are critics getting harsher? What books were released in 2013 and 2016 that made the difference in these years?

```{r}
books_clean %>% 
  group_by(publication_year) %>% 
  summarise(year_average_rating = mean(average_rating)) %>% 
  select(publication_year, year_average_rating) %>% 
  arrange(desc(year_average_rating))
```



Steps:
- group_by(year)
- mutate(year_average_rating = mean(average_rating))
#- arrange(desc(year_average_rating))
- slice_max(year_average_rating, n = 10)

# Finding 4
What is the trend in the number of reviews being completed over the last 10 years? 

NB- group by year, sum number of reviews in new column, use lag to create dif_in_reviews, arrange by year.

Steps:
- group_by(year)
- mutate(year_reviews_count = sum(text_reviews_count))
- mutate(ly_reviews_count = lag(year_reviews_count))
- mutate(yoy_reviews_count_change = year_reviews_count - ly_reviews_count)
- assign to new variable

- with new variable
- group_by(year)
- select(year, yoy_reviews_count_change)
- arrange(desc(year))
- head(10)

# Finding 5
Top 5 languages books have been published in, excluding English (GB or US).

Steps:
- filter(language_code != "en" | "eng-US)
- assign to new variable

- with new variable
- group_by(language_code)
- mutate(total_books_by_language = n())
- slice_max(total_books_by_language, n = 5)